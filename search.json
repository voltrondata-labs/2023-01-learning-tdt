[
  {
    "objectID": "01-part-1.html",
    "href": "01-part-1.html",
    "title": "Part 1 – Introduction",
    "section": "",
    "text": "Type in the chat: - what you know about Apache Arrow in one sentence\n\n\n\nDemonstration of what the training is working towards that shows the power of Arrow to work with large datasets. This should be live coded but not take more than 5 min."
  },
  {
    "objectID": "01-part-1.html#overview-of-the-training",
    "href": "01-part-1.html#overview-of-the-training",
    "title": "Part 1 – Introduction",
    "section": "Overview of the Training",
    "text": "Overview of the Training\n\nImporting data in the Apache Arrow ecosystems, what are the different file formats, how do they differ, which ones to use?\nLearning how to use Ibis for data wrangling on Parquet files\nUse Ibis with remote data sources: AWS S3 and others (Big Query? Postgres?)"
  },
  {
    "objectID": "01-part-1.html#presentation-of-the-dataset",
    "href": "01-part-1.html#presentation-of-the-dataset",
    "title": "Part 1 – Introduction",
    "section": "Presentation of the dataset",
    "text": "Presentation of the dataset\n\nWhat is iNaturalist?\nadd intro about project, show data on website\n\n\nThe dataset\nMore info: https://github.com/inaturalist/inaturalist-open-data/tree/main/Metadata\nData source: https://registry.opendata.aws/inaturalist-open-data/\n4 tables: - taxa.csv (most specialized) (153 MB): + taxon_id: unique identifier for a taxon + ancestry: classification for the taxon + rank_level: number associated with the rank. Parent have higher numbers than children + rank: Linnean ranks + name: name of the taxa\n[Illustrate with image that shows the full classification for a well known species, maybe Homo sapiens].\n\nobservers.csv (12 MB):\n\nobserver_id: unique identifier for the observer, can be used to construct URLs like https://inaturalist.org/users/1\nlogin: unique identifier for the observer\nname: personal name of the observer (if provided)\n\nphotos.csv (16 GB):\n\nphoto_uuid: unique identifier for the photo\nphoto_id: numeric identifier for the photo, can be used to construct URLs like https://inaturalist.org/photos/1010101\nobservation_uuid: unique identifier for the association observation\nextension: character, image file format\nlicence: license for the photo Creative Commons variant including CC0.\nwidth: number of pixels for the width of the image\nheight: number of pixels for the height of the image\nposition: order in which to display multiple photos of the same observation.\n\nobservations.csv (9 GB):\n\nobservation_uuid: unique identifier for the observation. Can be used to construct URLs like <https://inaturalist.org/observations/e041ae3d-1210-43a2-bdc2-8adb42b9e086\nobservation_id: identifier used on the iNaturalist website. Can be used to construct URLs like https://inaturalist.org/observations/1000000.\nlatitude: latitude for the observation\nlongitude: longitude for the observation\npositional_accuracy: uncertainty in meters around the latitude and longitude\ntaxon_id: the identifier of the taxon identified on the photo\nquality_grade: Casual observations are missing certain data components (e.g. latitude) or may have flags associated with them not shown here (e.g. location appears incorrect). Observations flagged as not wild are also considered Casual. All other observations are either Needs ID or Research Grade. Generally, Research Grade observations have more than one agreeing identifications at the species level, or if there are disagreements at least ⅔ of the identifications are in agreement a the species level.\nobserved_on: the date at which the observation took place\n\n\n[ Create a glossary with some terms related to the dataset? + taxon/taxa + ancestry (it has a meaning that may not be familiar in this context) + intro to Linnean classification]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is draft version of the content that will be presented during Learning at The Data Thread, a 2-day, hands-on workshop to get started using Arrow and Ibis for working with larger than memory datasets."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Learning at The Data Thread, January 2023",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "02-part-2.html",
    "href": "02-part-2.html",
    "title": "Part 2 – Data Import and File Formats",
    "section": "",
    "text": "Use the single file API\nInstructor first demonstrates on table ‘observers’.\n\nobservers <- read_tsv_arrow(\"~/datasets/inaturalist/csv/observers.csv\")\n\nExamine the output, point out:\n\ndata types\nmissing data\n\nActivity: participants do it on the taxa table (probably all together, no need for breakout rooms).\n\ntaxa <- read_tsv_arrow(\"~/datasets/inaturalists/csv/taxa.csv\")\n\n\n\n\nArrow is a data-format and a standard.\nPyArrow (and arrow R package) provides ways to interact with the data. However, while the arrow R package uses dplyr syntax to query data, the PyArrow syntax can become complicated for data manipulation. It is also possible to use pandas, but if the data is large, you will run into memory issues and the serialization/deserialization process can slow things down.\nFor this training, we will use Ibis. Ibis provides a consistent and expressive syntax, that supports multiple backends. For this training, we will rely on DuckDB as a backend. It supports many of the same data formats also supported by PyArrow making it easy (but also maybe confusing at times) to go from one to the other.\n[insert figure something like:\n\n\n                           PyArrow      ---------->    data files\n                                             /     (CSV, Arrow, Parquet)\n         Ibis     ---->    DuckDB       ----/\n]\nAdditionally, with Ibis, the code used for queries is independent from the backend. If the data are moved to a different system (PostgreSQL, SQLite, Google BigQuery, etc…), the code will not need to be rewritten.\nWe will start by using Ibis and DuckDB to query the data in the CSV files directly. Later we will work with larger data files where we can have much better performance by using file formats designed for query execution speed.\nInstructor demonstrates queries on the taxa table:\n\nfilter on Kingdom and active\nhow many “active” species (and lower ranks) are in the dataset?\nwhat are the names of the “active” species in the genus Panthera (tigers, lions, leopards, panthers, …).\n\n\n[here the goal would be for participants to start interacting with Arrow data while starting to learn about the structure of the data we will use in the training. With R, that would be the natural thing to do with the dplyr interface. To avoid introducing too many ways of interacting with data (and limit cognitive load), focus on working with Ibis. Would it be a good idea to add demonstrations on how to do some of these queries with pandas or PyArrow as well here just to show that it’s possible? and how the syntax compares for participants who will most likely be already familiar with Pandas?]\n[this needs to be translated to Ibis]\ntaxa %>%\n  filter(rank == \"kingdom\" & active)\n\ntaxa %>%\n  filter(active, rank_level <= 10) %>%\n  count()\n\ntaxa %>%\n  filter(\n    active,\n    rank == \"species\",\n    grepl(\"^Panthera\\\\s\", name)\n  ) %>%\n  select(name)\n\n\n\nwhat is taxon 48460? why is its ancestry NA?\nhow many animal species there is in the dataset?\n\n[this needs to be translated to Ibis]\ntaxa %>%\n  filter(taxon_id == 48460)\n\ntaxa %>%\n  filter(\n    active,\n    grepl(\"^48460/1/\", ancestry),\n    rank == \"species\"\n  ) %>%\n  count()      \n\n\n\n\n\nBefore we dive into using Arrow to read large data files, we need to do a tour of the different file formats supported by the Apache Arrow ecosystem.\nWe want to use an interactive approach here to avoid being repetitive. Assign each group to convert the taxa CSV file to:\n\narrow/feather\nparquet [- should ORC be included here?]\n\nusing the “single file” API.\nIntroduce way of keeping track of time for performance tracking, and how to measure disk usage of file.\nPoint to the API documentation to show syntax for how to create these files. Then ask:\n\nhow long does it take to read the files back into Arrow?\nhow much space do the files take on the hard drive?\nhow long does a query on these files take?\n\nFinalize exercise with creating a table to compare the different formats including CSV.\n\n\n\n\n\nWe have been using Arrow but have not seen its benefits yet for working with large data files, because the data is small enough that we could have used pandas to work with it.\nThe approach of using read_tsv_arrow() on large datasets does not work as it loads the full dataset in memory and the datasets are too large to fit in memory.\nWe need to use a different approach, the “Dataset API”.\nRead CSV photos file with dataset API. Show memory usage (if interfaces allows us to do it easily).\n\n\nimport ibis\nimport time\n\nibis.options.interactive = True\n\ncon = ibis.duckdb.connect()\ncon.register(\"~/datasets/inaturalist/csv/photos.csv\", table_name = \"photos\")\n\n## this is fast because of deferred evaluation\ntic = time.perf_counter()\nfreq_extensions = con.table(\"photos\").extension.value_counts()\ntoc = time.perf_counter()\nprint(f\"Counting unique values from the CSV file: {toc - tic:0.4f} seconds\")\n\n## actually printing forces the computation and takes about 100 seconds\ntic = time.perf_counter()\nfreq_extensions = con.table(\"photos\").extension.value_counts()\nprint(freq_extensions)\ntoc = time.perf_counter()\nprint(f\"Counting unique values from the CSV file: {toc - tic:0.4f} seconds\")\n\nLet’s convert to a single Parquet file to see what happens:\n\nimport pyarrow as pa\nimport pyarrow.csv\nimport pyarrow.parquet as pq\nimport time\n\nin_path = '~/datasets/inaturalist/csv/photos.csv'\nout_path = '~/datasets/inaturalist/photos_python.parquet'\n\nparse_options = pyarrow.csv.ParseOptions(delimiter=\"\\t\")\n\ntic = time.perf_counter()\nwriter = None\nwith pyarrow.csv.open_csv(in_path, parse_options = parse_options) as reader:\n    for next_chunk in reader:\n        if next_chunk is None:\n            break\n        if writer is None:\n            writer = pq.ParquetWriter(out_path, next_chunk.schema)\n        next_table = pa.Table.from_batches([next_chunk])\n        writer.write_table(next_table)\nwriter.close()\ntoc = time.perf_counter()\nprint(f\"Converted file in {toc - tic:0.4f} seconds\")\n\nThe conversation takes 190 seconds. Let’s redo a query (still using Ibis) on it.\ncon.register(\"/home/francois/datasets/inat/photos_python.parquet\", table_name = \"photos_parquet\")\n\ntic = time.perf_counter()\nfreq_extensions_pq = con.table(\"photos_parquet\").extension.value_counts()\nprint(freq_extensions_pq)\ntoc = time.perf_counter()\nprint(f\"Counting unique values from single parquet file: {toc - tic:0.4f} seconds\")\nMuch faster. 1.2 seconds. Can we go even faster?\n[ need to work on transition here ]\nLet’s create a collection of parquet files. Typically, the files are partionned along a variable, but in this table, there is not an obvious variable we can use. Instead we are going to partition the data based on a maximum number of rows. And instead of reading in chunks manually by writing a for loop, we can also use the Dataset API to read the CSV file.\nimport pyarrow\nimport pyarrow.csv\nimport pyarrow.dataset as ds\nimport time\n\nin_path = \"~/datasets/inaturalist/csv/photos.csv\"\nout_path = \"~/datasets/inaturalist/photos_python_table/\"\nparse_options = pyarrow.csv.ParseOptions(delimiter = \"\\t\")\n\n## read the CSV file using the dataset API\ntic = time.perf_counter()\nphotos = ds.dataset(in_path,\n                    format = ds.CsvFileFormat(\n                        parse_options = parse_options\n                    ))\ntoc = time.perf_counter()\nprint(f\"Reading CSV file in {toc - tic:0.4f} seconds\")\n\n## Convert the CSV file to a collection of Parquet files\ntic = time.perf_counter()\nds.write_dataset(photos, out_path,\n                 format=\"parquet\", max_rows_per_file = 1e7)\ntoc = time.perf_counter()\nprint(f\"Writing parquet file in {toc - tic:0.4f} seconds\")\n\n## Read the collection of Parquet files\ntic = time.perf_counter()\nphotos = ds.dataset(out_path,\n                    format = \"parquet\")\ntoc = time.perf_counter()\nprint(f\"loading parquet file system in {toc - tic:0.4f} seconds\")\n\n## Query this system of Parquet files\ncon.register(\"parquet:///~/datasets/inaturalist/photos_parquet/*.parquet\", table_name = \"photos_multi_pq\")\ntic = time.perf_counter()\nprint(con.table(\"photos_multi_pq\").extension.value_counts())\ntoc = time.perf_counter()\nprint(f\"Group by and sum with Ibis in {toc - tic:0.4f} seconds\")\nThis takes also about 1.2 seconds. The conversion to multiple Parquet files was faster.\n[note that with this approach we don’t show that the Dataset API can read multiple CSV files but we do demonstrate how to do this on multiple parquet files.]\n\n\n\nRedo the same as for the photos csv file but with the observation dataset: - read the CSV file using the Dataset API - extract the year and month columns from the dataset - write to multi parquet filesystem partioned by year\nobservations_csv = \"//home/francois/datasets/inat/inaturalist-open-data-20220827/observations.csv\"\n\ncon.register(observations_csv, table_name = \"observations_csv\")\n\nt = con.table(\"observations_csv\")\n\nwith_dates = t.mutate(month = t.observed_on.month(), year = t.observed_on.year())\n\nobservations = ds.dataset(\n     observations_csv,\n     format = ds.CsvFileFormat(\n          parse_options = pyarrow.csv.ParseOptions(delimiter = \"\\t\")\n     )\n)"
  }
]